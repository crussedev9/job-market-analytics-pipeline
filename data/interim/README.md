# Interim Data Folder

This folder contains intermediate, cleaned, and transformed CSV files produced by the Python ETL scripts.

## Purpose

The interim folder serves as a staging area between raw data ingestion and final SQL database loading. Files here have been cleaned and normalized but not yet loaded into the dimensional model.

## Auto-Generated Files

These files are created automatically by running the Python scripts:

### From `01_ingest_raw_files.py`:
- `job_postings_raw_combined.csv` - Combined raw data from all source files

### From `02_clean_and_normalize.py`:
- `job_postings_cleaned.csv` - Cleaned fact table base with normalized columns
- `dim_job.csv` - Unique job titles with derived attributes
- `dim_company.csv` - Unique company records
- `dim_location.csv` - Unique location records with parsed city/state/country
- `dim_employment_type.csv` - Unique employment type and work arrangement combinations
- `dim_skill.csv` - Unique skills extracted from job descriptions
- `bridge_posting_skill.csv` - Many-to-many mapping between postings and skills

## Important Notes

- **Do NOT edit these files manually** - they are auto-generated by the ETL pipeline
- These files are **not tracked in Git** (see `.gitignore`)
- If you need to make changes, modify the Python scripts and re-run the pipeline
- These CSVs serve as inputs to the SQL loading step (`03_export_for_sql_load.py`)

## Data Lineage

```
data/raw/job_postings_raw.csv
    ↓ (01_ingest_raw_files.py)
data/interim/job_postings_raw_combined.csv
    ↓ (02_clean_and_normalize.py)
data/interim/[dimensional CSVs listed above]
    ↓ (03_export_for_sql_load.py)
SQLite database or data/processed/
```

## Troubleshooting

If files in this folder appear corrupted or incomplete:
1. Delete all files in `data/interim/`
2. Re-run the Python scripts starting from `01_ingest_raw_files.py`
3. Check the console output for any errors or warnings
